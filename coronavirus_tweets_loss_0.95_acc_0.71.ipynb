{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oYwKJWQ-ReA",
        "outputId": "9b3eb38a-5bf6-486e-e35a-9220e31d9007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.25.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Uw1RsOO0iA",
        "outputId": "647c9c62-60c9-403e-cd44-a8a0ec7ffe4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV5rZ2BV-d2t"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "root_dir = '/content'\n",
        "data_dir = '/content/corona'\n",
        "\n",
        "if os.path.exists(data_dir):\n",
        "  shutil.rmtree(data_dir)\n",
        "\n",
        "with zipfile.ZipFile(os.path.join(root_dir,'drive/MyDrive/data/corona_tweets.zip'),'r') as file:\n",
        "  file.extractall(data_dir)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfwjRbm-_2dW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(os.path.join(data_dir,'Corona_NLP_train.csv'),encoding='latin-1')\n",
        "train_data = train_data[['OriginalTweet','Sentiment']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAIPJE5CPZi8",
        "outputId": "0f71b5ac-fd28-4241-b0bc-1b8a482c6576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                       OriginalTweet           Sentiment\n",
            "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
            "1  advice Talk to your neighbours family to excha...            Positive\n",
            "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
            "3  My food stock is not the only one which is emp...            Positive\n",
            "4  Me, ready to go at supermarket during the #COV...  Extremely Negative\n"
          ]
        }
      ],
      "source": [
        "print(train_data.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YjeZUweH6wx",
        "outputId": "b70d9eb1-ffda-408e-8c17-902ef0a21d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(41157, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWNG6ITIL2PL",
        "outputId": "c6222a34-67b6-46c1-e41f-8d793ab86d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    OriginalTweet\n",
            "Sentiment                        \n",
            "Extremely Negative           5481\n",
            "Extremely Positive           6624\n",
            "Negative                     9917\n",
            "Neutral                      7713\n",
            "Positive                    11422\n"
          ]
        }
      ],
      "source": [
        "print(train_data.groupby('Sentiment').count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY7G9GqyONpH",
        "outputId": "3c63d751-589d-4b7d-b9bb-c27567780068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    2\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    4\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "train_data['Sentiment'] = train_data['Sentiment'].replace({'Extremely Positive': 0,'Positive': 1,\n",
        "                                                           'Neutral': 2, 'Negative': 3, 'Extremely Negative': 4})\n",
        "\n",
        "print(train_data['Sentiment'].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMeYp3c6Bv67",
        "outputId": "ac71843a-7015-4d1c-c8ef-40dfdaa76ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OriginalTweet    0\n",
            "Sentiment        0\n",
            "dtype: int64 \n",
            "\n",
            "False    41157\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data.isnull().sum(),'\\n')\n",
        "\n",
        "print(train_data.duplicated(subset = ['OriginalTweet']).value_counts())\n",
        "\n",
        "train_data.drop_duplicates(subset = 'OriginalTweet',inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFTaElbs-ctZ"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(os.path.join(data_dir,'Corona_NLP_test.csv'),encoding='latin-1')\n",
        "test_data = test_data[['OriginalTweet','Sentiment']]\n",
        "\n",
        "test_data['Sentiment'] = test_data['Sentiment'].replace({'Extremely Positive': 0,'Positive': 1,\n",
        "                                                           'Neutral': 2, 'Negative': 3, 'Extremely Negative': 4})\n",
        "\n",
        "test_data.drop_duplicates(subset = 'OriginalTweet',inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coeO8A2dBF7H",
        "outputId": "ccc7fd57-1dfc-43d9-905b-b9c706a657e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(37041, 2) (4116, 2) (3798, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "rs = [45,35,120,37,434,293]\n",
        "\n",
        "train_data,val_data = train_test_split(train_data,test_size = 0.1,random_state = rs[2],stratify = train_data['Sentiment'])\n",
        "\n",
        "train_data.reset_index(drop = True, inplace = True)\n",
        "val_data.reset_index(drop = True, inplace = True)\n",
        "test_data.reset_index(drop = True, inplace = True)\n",
        "\n",
        "print(train_data.shape,val_data.shape,test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtOUjPsjHE2Y",
        "outputId": "66742ef9-de8a-403e-c36b-7ca007a36477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                           OriginalTweet  Sentiment\n",
            "37021  While there may be short term effect due to lo...          0\n",
            "37022  MOL, a Hungary based lube manufacturer, has tr...          2\n",
            "37023  From our COVid19 resource guide, Interim Guida...          1\n",
            "37024  Critter Sitters is now offering Grocery Shoppi...          1\n",
            "37025  List of aisles empty at the grocery store: 1) ...          3\n",
            "37026  #Germany #Coronavirus\\r\\r\\n\\r\\r\\nConsumer advo...          4\n",
            "37027  So I wiped the cart down but then I touched th...          0\n",
            "37028  These #charges are never in favour of the cons...          3\n",
            "37029  Chicago consumers be aware of consumer fraud r...          4\n",
            "37030  ?? Weaker consumer confidence \\r\\r\\n?? High un...          4\n",
            "37031  sigh RIP online shopping for the next few mont...          2\n",
            "37032  Covid-19 lockdown sucks. But these gas prices,...          3\n",
            "37033  @Bogs4NY They're escaping NY and invading thei...          1\n",
            "37034  President Trump just said, ÂWe donÂt have em...          2\n",
            "37035  Explosive Silver Prices Will Be Mind Boggling\\...          2\n",
            "37036  Coronavirus: 20% plummet in house prices predi...          2\n",
            "37037  @RefinitivAgri\\r\\r\\n#India is likely to export...          3\n",
            "37038  Japanese Supermarket Now after Corona Emergenc...          3\n",
            "37039  Just when you thought people couldn't get more...          4\n",
            "37040  When you go to a shop to buy 10kg of pasta and...          3 \n",
            "\n",
            "                                           OriginalTweet  Sentiment\n",
            "37021  While there may be short term effect due to lo...          0\n",
            "37022  MOL a Hungary based lube manufacturer has tran...          2\n",
            "37023  From our COVid19 resource guide Interim Guidan...          1\n",
            "37024  Critter Sitters is now offering Grocery Shoppi...          1\n",
            "37025  List of aisles empty at the grocery store 1 to...          3\n",
            "37026    advocates have warned of a massive surge in ...          4\n",
            "37027  So I wiped the cart down but then I touched th...          0\n",
            "37028  These  are never in favour of the consumer Egs...          3\n",
            "37029  Chicago consumers be aware of consumer fraud r...          4\n",
            "37030   Weaker consumer confidence  High unemployment...          4\n",
            "37031  sigh RIP online shopping for the next few mont...          2\n",
            "37032  Covid19 lockdown sucks. But these gas prices I...          3\n",
            "37033   They're escaping NY and invading their summer...          1\n",
            "37034  President Trump just said ÂWe donÂt have empty...          2\n",
            "37035  Explosive Silver Prices Will Be Mind BogglingB...          2\n",
            "37036  Coronavirus 20 plummet in house prices predict...          2\n",
            "37037   is likely to export 4.5Mn T of  in 201920  15...          3\n",
            "37038  Japanese Supermarket Now after Corona Emergenc...          3\n",
            "37039  Just when you thought people couldn't get more...          4\n",
            "37040  When you go to a shop to buy 10kg of pasta and...          3\n"
          ]
        }
      ],
      "source": [
        "#preprocess\n",
        "\n",
        "import re\n",
        "\n",
        "def preprocessing(text):\n",
        "  com = re.compile(\"[^\\w\\d'\\. ]\")\n",
        "  http = re.compile('https://[^ ]+(/[^ ])+')\n",
        "  tag = re.compile('@[^ ]+')\n",
        "  tag2 = re.compile('#[^ ]+')\n",
        "  text = http.sub('',text)\n",
        "  text = tag.sub('',text)\n",
        "  text = tag2.sub('',text)\n",
        "  text = com.sub('',text)\n",
        "\n",
        "  return text\n",
        "\n",
        "print(train_data.tail(20),'\\n')\n",
        "\n",
        "train_data['OriginalTweet'] = train_data['OriginalTweet'].apply(func = preprocessing)\n",
        "val_data['OriginalTweet'] = val_data['OriginalTweet'].apply(func = preprocessing)\n",
        "test_data['OriginalTweet'] = test_data['OriginalTweet'].apply(func = preprocessing)\n",
        "\n",
        "print(train_data.tail(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njKA77abyhSR"
      },
      "outputs": [],
      "source": [
        "from transformers.pipelines import text_classification\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "pre_train = ['bert-base-uncased','bert-large-uncased']\n",
        "\n",
        "Tokenizer = BertTokenizer.from_pretrained(pre_train[0])\n",
        "LemMatizer = WordNetLemmatizer()\n",
        "Stopword = stopwords.words('english')\n",
        "\n",
        "def bertTokenizer(text):\n",
        "  return Tokenizer.tokenize(text)\n",
        "\n",
        "def LemMatize(text):\n",
        "  new_text = []\n",
        "  for t in text:\n",
        "    new_text.append(LemMatizer.lemmatize(t))\n",
        "  return new_text\n",
        "\n",
        "def clear_noise(text):\n",
        "  texts = []\n",
        "  alp = re.compile('[a-zA-Z0-9.]')\n",
        "  for t in text:\n",
        "    if alp.fullmatch(t) == None:\n",
        "      texts.append(t)\n",
        "  return texts\n",
        "\n",
        "def stopwords(text):\n",
        "  texts = []\n",
        "  for t in text:\n",
        "    if t not in Stopword:\n",
        "      texts.append(t)\n",
        "  return texts\n",
        "\n",
        "def preprocess(text):\n",
        "  text = bertTokenizer(text)\n",
        "  #text = LemMatize(text)\n",
        "  #text = clear_noise(text)\n",
        "  #text = stopwords(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "def make_sentence(text):\n",
        "  return ' '.join(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmcdWquY7196"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data.OriginalTweet = train_data.OriginalTweet.apply(func = preprocess)\n",
        "val_data.OriginalTweet = val_data.OriginalTweet.apply(func = preprocess)\n",
        "test_data.OriginalTweet = test_data.OriginalTweet.apply(func = preprocess)\n",
        "\n",
        "print(train_data['OriginalTweet'].head(20),'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv-bUjGUpty9"
      },
      "outputs": [],
      "source": [
        "def to_id(text):\n",
        "  return Tokenizer.convert_tokens_to_ids(text)\n",
        "\n",
        "train_data.OriginalTweet = train_data.OriginalTweet.apply(func = to_id)\n",
        "val_data.OriginalTweet = val_data.OriginalTweet.apply(func = to_id)\n",
        "test_data.OriginalTweet = test_data.OriginalTweet.apply(func = to_id)\n",
        "\n",
        "print(train_data['OriginalTweet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbWqq8Nktely"
      },
      "outputs": [],
      "source": [
        "#padding\n",
        "\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "applen = train_data.OriginalTweet.apply(func = len)\n",
        "max_len = applen.max()\n",
        "avg_len = applen.mean()\n",
        "print(max_len)\n",
        "print(avg_len)\n",
        "\n",
        "x_train = pad_sequences(train_data.OriginalTweet,maxlen=max_len,padding='pre')\n",
        "x_val = pad_sequences(val_data.OriginalTweet,maxlen=max_len,padding='pre')\n",
        "x_test = pad_sequences(test_data.OriginalTweet,maxlen=max_len,padding='pre')\n",
        "\n",
        "print(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCAwGyI2L3uG"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_train = train_data.Sentiment\n",
        "y_val = val_data.Sentiment\n",
        "y_test = test_data.Sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXSm7iXJmrLi"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import Sequential, regularizers\n",
        "from tensorflow.keras.layers import Dense, Activation, LSTM, Embedding, GRU\n",
        "\n",
        "word_count = Tokenizer.vocab_size\n",
        "label_count = len(train_data['Sentiment'].unique())\n",
        "print(word_count)\n",
        "print(label_count)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(word_count+1,64,input_length = max_len,embeddings_regularizer = regularizers.L2(0.002)),\n",
        "    #LSTM(140,dropout = 0.2,activation='tanh'),\n",
        "    GRU(140,dropout = 0.2,activation='tanh'),\n",
        "    Dense(label_count,activation = 'softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj8Sl2174xLD"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TkxBvLTg5NOh",
        "outputId": "01476c90-620d-41ac-feb8-41dc91081e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1158/1158 [==============================] - 14s 9ms/step - loss: 2.0061 - accuracy: 0.2840 - val_loss: 1.5251 - val_accuracy: 0.3112 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 1.4740 - accuracy: 0.3368 - val_loss: 1.4337 - val_accuracy: 0.3598 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "1158/1158 [==============================] - 10s 9ms/step - loss: 1.3567 - accuracy: 0.4181 - val_loss: 1.3420 - val_accuracy: 0.4244 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 1.2774 - accuracy: 0.4578 - val_loss: 1.3084 - val_accuracy: 0.4456 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "1158/1158 [==============================] - 10s 9ms/step - loss: 1.2354 - accuracy: 0.4831 - val_loss: 1.3035 - val_accuracy: 0.4594 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "1158/1158 [==============================] - 10s 9ms/step - loss: 1.2054 - accuracy: 0.5027 - val_loss: 1.2898 - val_accuracy: 0.4779 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 1.1674 - accuracy: 0.5306 - val_loss: 1.2219 - val_accuracy: 0.5267 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 1.0716 - accuracy: 0.6015 - val_loss: 1.1084 - val_accuracy: 0.5943 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "1158/1158 [==============================] - 10s 9ms/step - loss: 0.9389 - accuracy: 0.6636 - val_loss: 1.0190 - val_accuracy: 0.6263 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.8320 - accuracy: 0.7152 - val_loss: 0.9229 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.7545 - accuracy: 0.7524 - val_loss: 0.8642 - val_accuracy: 0.7089 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.6990 - accuracy: 0.7808 - val_loss: 0.8522 - val_accuracy: 0.7128 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.6625 - accuracy: 0.7966 - val_loss: 0.8573 - val_accuracy: 0.7182 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.6299 - accuracy: 0.8151 - val_loss: 0.8554 - val_accuracy: 0.7138 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.5537 - accuracy: 0.8476 - val_loss: 0.8309 - val_accuracy: 0.7369 - lr: 2.0000e-05\n",
            "Epoch 16/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.5302 - accuracy: 0.8558 - val_loss: 0.8324 - val_accuracy: 0.7383 - lr: 2.0000e-05\n",
            "Epoch 17/20\n",
            "1158/1158 [==============================] - 11s 10ms/step - loss: 0.5184 - accuracy: 0.8592 - val_loss: 0.8309 - val_accuracy: 0.7371 - lr: 2.0000e-05\n",
            "Epoch 18/20\n",
            "1158/1158 [==============================] - 10s 9ms/step - loss: 0.5034 - accuracy: 0.8659 - val_loss: 0.8425 - val_accuracy: 0.7413 - lr: 2.0000e-05\n",
            "Epoch 19/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.4941 - accuracy: 0.8693 - val_loss: 0.8485 - val_accuracy: 0.7403 - lr: 2.0000e-05\n",
            "Epoch 20/20\n",
            "1158/1158 [==============================] - 10s 8ms/step - loss: 0.4875 - accuracy: 0.8694 - val_loss: 0.8465 - val_accuracy: 0.7388 - lr: 2.0000e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f76314d3950>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',patience = 4,restore_best_weights = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, factor = 0.2,cooldown = 5)\n",
        "\n",
        "model.fit(x_train,y_train,validation_data=(x_val,y_val),epochs = 20,batch_size = 32,callbacks = [earlystop,reduce_lr])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8zFbsv_4_CEk",
        "outputId": "d15b5de7-0590-4653-b255-652bf0440d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "119/119 [==============================] - 1s 5ms/step - loss: 0.9544 - accuracy: 0.7117\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.954431414604187, 0.711690366268158]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model.evaluate(x_test,y_test)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1_zmdiUrLMxhtFgnd5TscLRONfkyhMeDX",
      "authorship_tag": "ABX9TyN6EIblQPO47IeJNMeVEJF1"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
